<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Cianna Bedford-Petersen</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 02 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_huceb56bb3afe6c2d3e77d7e55f8d57a96_326227_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Tidymodels: Decision Tree Learning in R</title>
      <link>/post/2020-06-22-tidymodels-decision-tree-learning-in-r/</link>
      <pubDate>Tue, 02 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-06-22-tidymodels-decision-tree-learning-in-r/</guid>
      <description>


&lt;div id=&#34;intro&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intro&lt;/h1&gt;
&lt;p&gt;Tidyverse’s newest release has recently come together to form a cohesive suite of packages for modeling and machine learning, called &lt;code&gt;{tidymodels}&lt;/code&gt;. The successor to Max Kuhn’s &lt;code&gt;{caret}&lt;/code&gt; package, &lt;code&gt;{tidymodels}&lt;/code&gt; allows for a tidy approach to your data from start to finish. We’re going to walk through the basics for getting off the ground with &lt;code&gt;{tidymodels}&lt;/code&gt; and demonstrate its application to three different tree-based methods for predicting student test scores. For further information about the package, you can visit &lt;a href=&#34;https://www.tidymodels.org/&#34; class=&#34;uri&#34;&gt;https://www.tidymodels.org/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Setup&lt;/h1&gt;
&lt;p&gt;Load both the &lt;code&gt;{tidyverse}&lt;/code&gt; and &lt;code&gt;{tidymodels}&lt;/code&gt; packages into your environment. We’ll also load in the &lt;code&gt;{skimr}&lt;/code&gt; package to help us with some descriptives for our data and a host of other packages that will be required to run our machine learning models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidymodels) 
library(tidyverse) # manipulating data
library(skimr) # data visualization
library(baguette) # bagged trees
library(future) # parallel processing &amp;amp; decrease computation time
library(xgboost) # boosted trees&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;import-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Import the data&lt;/h1&gt;
&lt;p&gt;We’re using simulated data which approximates reading and math scores for ~189,000 3rd-8th grade students in Oregon public schools see &lt;a href=&#34;https://www.kaggle.com/c/edld-654-spring-2020&#34;&gt;this Kaggle page&lt;/a&gt; for details. For the purpose of demonstration, we’ll be sampling 1% of the data with &lt;code&gt;sample_frac()&lt;/code&gt; to keep computer processing time manageable. All school IDs in the data are real, so we can use that information to link the data with other sources. Specifically, we’re also going to pull in some data on student enrollment in free and reduced lunch from the National Center for Education Statistics and some ethnicity data from the Oregon Department of Education.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(100)

# import data and perform initial cleaning
# initial cleaning steps include: 
# *recode NA&amp;#39;s for lang_cd and ayp_lep to more meaningful values
# *remove vars with entirely missing data
# Note: the data is called &amp;#39;train.csv&amp;#39;, but we will actually further split this into its own training and testing data

dat &amp;lt;- read_csv(here::here(&amp;quot;static&amp;quot;, &amp;quot;data&amp;quot;, &amp;quot;train.csv&amp;quot;)) %&amp;gt;% 
  select(-classification) %&amp;gt;% # remove this variable because it&amp;#39;s redundant with `score`
  mutate(lang_cd = ifelse(is.na(lang_cd), &amp;quot;E&amp;quot;, lang_cd), 
         ayp_lep = ifelse(is.na(ayp_lep), &amp;quot;G&amp;quot;, ayp_lep)) %&amp;gt;% 
  sample_frac(.01) %&amp;gt;% # sample 1% of the data to reduce run time
  janitor::remove_empty(c(&amp;quot;rows&amp;quot;, &amp;quot;cols&amp;quot;)) %&amp;gt;% 
  drop_na() %&amp;gt;% 
  select_if(~length(unique(.x)) &amp;gt; 1)

# import fall membership report ethcnicity data and do some basic cleaning and renaming
sheets &amp;lt;- readxl::excel_sheets(here::here(&amp;quot;static&amp;quot;, &amp;quot;data&amp;quot;, &amp;quot;fallmembershipreport_20192020.xlsx&amp;quot;))

ode_schools &amp;lt;- readxl::read_xlsx(here::here(&amp;quot;static&amp;quot;, &amp;quot;data&amp;quot;, &amp;quot;fallmembershipreport_20192020.xlsx&amp;quot;),
                                 sheet = sheets[4])

ethnicities &amp;lt;- ode_schools %&amp;gt;%
  select(attnd_schl_inst_id = `Attending School ID`,
         attnd_dist_inst_id = `Attending District Institution ID`,
         sch_name = `School Name`,
         contains(&amp;quot;%&amp;quot;)) %&amp;gt;%
  janitor::clean_names()

names(ethnicities) &amp;lt;- gsub(&amp;quot;x2019_20_percent&amp;quot;, &amp;quot;p&amp;quot;, names(ethnicities))

# join ethnicity data with original dataset
dat &amp;lt;- left_join(dat, ethnicities)

# import and tidy free and reduced lunch data 
frl &amp;lt;- rio::import(&amp;quot;https://nces.ed.gov/ccd/Data/zip/ccd_sch_033_1718_l_1a_083118.zip&amp;quot;,
              setclass = &amp;quot;tbl_df&amp;quot;) %&amp;gt;% 
  janitor::clean_names()  %&amp;gt;% 
  filter(st == &amp;quot;OR&amp;quot;)  %&amp;gt;%
  select(ncessch, lunch_program, student_count)  %&amp;gt;% 
  mutate(student_count = replace_na(student_count, 0))  %&amp;gt;% 
  pivot_wider(names_from = lunch_program,
            values_from = student_count)  %&amp;gt;% 
  janitor::clean_names()  %&amp;gt;% 
  mutate(ncessch = as.double(ncessch))

# import student counts for each school across grades
stu_counts &amp;lt;- rio::import(&amp;quot;https://github.com/datalorax/ach-gap-variability/raw/master/data/achievement-gaps-geocoded.csv&amp;quot;, setclass = &amp;quot;tbl_df&amp;quot;)  %&amp;gt;% 
  filter(state == &amp;quot;OR&amp;quot; &amp;amp; year == 1718)  %&amp;gt;% 
  count(ncessch, wt = n)  %&amp;gt;% 
  mutate(ncessch = as.double(ncessch))

# join frl and stu_counts data
frl &amp;lt;- left_join(frl, stu_counts)

# add frl data to train data
dat &amp;lt;- left_join(dat, frl)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After loading in our three datasets, we’ll join them together to make one cohesive data set to use for modeling. After joining, the data contains both student-level variables (e.g. gender, ethnicity, enrollment in special education/talented and gifted programs, etc.) and district-level variables (e.g. school longitude and latitude, proportion of students who qualify for free and reduced-price lunch, etc.), all of which will be included for each 3 of our &lt;code&gt;{tidymodels}&lt;/code&gt; tree-based examples.&lt;/p&gt;
&lt;p&gt;For a more complete description of the variables, you can download the data dictionary &lt;a href=&#34;data_dictionary.csv&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;explore-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Explore the data&lt;/h1&gt;
&lt;p&gt;We’ll use the &lt;code&gt;skim()&lt;/code&gt; function from &lt;code&gt;{skimr}&lt;/code&gt; to take a closer look at our variables. Many numeric predictors are clearly non-normal (see histograms below), but this is no problem as tree-based methods are robust to non-normality.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% 
  select(-contains(&amp;quot;id&amp;quot;), -ncessch, -missing, -not_applicable) %&amp;gt;%  # remove ID and irrelevant variables
  mutate(tst_dt = lubridate::as_date(lubridate::mdy_hms(tst_dt))) %&amp;gt;% # covert test date to date
  modify_if(is.character, as.factor) %&amp;gt;%  # convert character vars to factors
  skim() %&amp;gt;% 
  select(-starts_with(&amp;quot;numeric.p&amp;quot;)) # remove quartiles&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-4&#34;&gt;Table 1: &lt;/span&gt;Data summary&lt;/caption&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Name&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Piped data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Number of rows&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1857&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Number of columns&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;41&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;_______________________&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Column type frequency:&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Date&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;factor&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;25&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;numeric&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;________________________&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Group variables&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;None&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Variable type: Date&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;skim_variable&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_missing&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;complete_rate&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;min&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;max&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;median&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_unique&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;tst_dt&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2018-03-16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2018-06-07&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2018-05-18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;47&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Variable type: factor&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;skim_variable&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_missing&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;complete_rate&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;ordered&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_unique&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;top_counts&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;gndr&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;M: 939, F: 918&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ethnic_cd&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;W: 1151, H: 458, M: 100, A: 79&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;tst_bnch&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;G6: 343, 1B: 330, G4: 304, G7: 304&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;migrant_ed_fg&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;N: 1793, Y: 64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ind_ed_fg&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;N: 1842, Y: 15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;sp_ed_fg&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;N: 1614, Y: 243&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;tag_ed_fg&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;N: 1759, Y: 98&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;econ_dsvntg&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1100, N: 757&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ayp_lep&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;G: 1471, F: 164, Y: 72, E: 58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;stay_in_dist&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1811, N: 46&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;stay_in_schl&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1803, N: 54&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;dist_sped&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;N: 1846, Y: 11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;trgt_assist_fg&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;N: 1773, Y: 83, y: 1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ayp_schl_partic&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1846, N: 11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ayp_dist_prfrm&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1803, N: 54&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ayp_schl_prfrm&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1785, N: 72&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;rc_schl_partic&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1846, N: 11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;rc_dist_prfrm&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1803, N: 54&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;rc_schl_prfrm&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1785, N: 72&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;lang_cd&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E: 1815, S: 42&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;tst_atmpt_fg&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1853, P: 4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;grp_rpt_schl_partic&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1846, N: 11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;grp_rpt_dist_prfrm&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1845, N: 12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;grp_rpt_schl_prfrm&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1834, N: 23&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;sch_name&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;699&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Hig: 14, Jud: 14, Hou: 13, Fiv: 11&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Variable type: numeric&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;skim_variable&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_missing&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;complete_rate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;sd&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;hist&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;enrl_grd&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.44&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.69&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▃▅▃▃&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;score&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2495.34&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;115.19&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▁▁▂▇▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;lat&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;44.79&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.99&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▂▁▂▅▇&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;lon&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-122.51&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▅▇▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p_american_indian_alaska_native&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.06&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p_asian&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.04&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.07&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p_native_hawaiian_pacific_islander&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p_black_african_american&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.02&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.04&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p_hispanic_latino&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.18&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▅▂▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p_white&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.60&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.20&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▁▃▅▇▅&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p_multiracial&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.06&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▆▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;free_lunch_qualified&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;231.23&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;147.55&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▇▃▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;reduced_price_lunch_qualified&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;39.86&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;24.77&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▆▇▃▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;no_category_codes&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;271.09&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;165.44&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▆▇▃▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;n&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;816.07&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;536.55&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▃▂▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;While most of our predictors are categorical, we can use &lt;code&gt;{corrplot}&lt;/code&gt; to better visualize the relationships among the numeric variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% 
  select(-contains(&amp;quot;id&amp;quot;), -ncessch, -missing, -not_applicable) %&amp;gt;% 
  select_if(is.numeric) %&amp;gt;% 
  select(score, everything()) %&amp;gt;% 
  cor(use = &amp;quot;pairwise.complete.obs&amp;quot;) %&amp;gt;% 
  corrplot::corrplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-06-22-tidymodels-decision-tree-learning-in-r/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;split-data-and-resample&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Split data and resample&lt;/h1&gt;
&lt;p&gt;The first step of our analysis is to split our data into two separate sets: a “training” set and a “testing” set. The training set is used to train a model and, if desired, to adjust (i.e., “tune”) the model’s hyperparameters before evaluating its final performance on our test data. By allowing us to test a model on a new sample, we assess “out of sample” accuracy (i.e., unseen data-—what all predictive models are interested in) and limit overfitting to the training set. We can do this efficiently with the &lt;code&gt;initial_split()&lt;/code&gt; function. This comes from the &lt;code&gt;{rsample}&lt;/code&gt; package, which is part of the &lt;code&gt;{tidymodels}&lt;/code&gt; package that we already loaded. Defaults put 75% of the data in the training set and 25% in the test set, but this can be adjusted with the &lt;code&gt;prop&lt;/code&gt; argument. Then, we’ll extract the training data from our split object and assign it a name.&lt;/p&gt;
&lt;p&gt;To further prevent over-fitting, we’ll resample our data using &lt;code&gt;vfold_cv()&lt;/code&gt;. This function outputs k-&lt;em&gt;fold&lt;/em&gt; cross-validated versions of our training data, where k = the number of times we resample (unsure why v- is used instead of k- here). By using k = 10 data sets, we get a better estimate of the model’s out-of-sample accuracy. On top of decreasing bias from over-fitting, this is essential when tuning hyperparameters (though we plan to apply defaults and not tune here, for brevity). Though our use of 10-fold cross validation is both frequently used and effective, it should be noted that other methods (e.g., bootstrap resampling) or other k-values are sometimes used to accomplish the same goal.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(100)

# split the data
split &amp;lt;- initial_split(dat)

# extract the training data
train &amp;lt;- training(split)

# resample the data with 10-fold cross-validation (10-fold by default)
cv &amp;lt;- vfold_cv(train)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pre-processing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Pre-processing&lt;/h1&gt;
&lt;p&gt;Before we add in our data to the model, we’re going to set up an object that pre-processes our data. This is called a &lt;em&gt;recipe&lt;/em&gt;. To create a recipe, you’ll first specify a formula for your model, indicating which variable is your outcome and which are your predictors. Using &lt;code&gt;~.&lt;/code&gt; here will indicate that we want to use all variables other than &lt;code&gt;score&lt;/code&gt; as predictors. Then, we can specify a series of pre-processing steps for our data that directs our recipe to assign our variables a role or performs feature engineering steps. Pre-processing may be sound uncommon, but if you’ve ever used &lt;code&gt;lm()&lt;/code&gt; (or several other &lt;code&gt;R&lt;/code&gt; functions) you’ve done some of this by simply calling the function (e.g., automatic dummy-coding to handle categorical data). This is beneficial because it gives the analyst more control, despite adding complexity to the process.&lt;/p&gt;
&lt;p&gt;A complete list of possible pre-processing steps can be found here: &lt;a href=&#34;https://recipes.tidymodels.org/articles/Custom_Steps.html&#34; class=&#34;uri&#34;&gt;https://recipes.tidymodels.org/articles/Custom_Steps.html&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rec &amp;lt;- recipe(score ~ ., train) %&amp;gt;% 
  step_mutate(tst_dt = as.numeric(lubridate::mdy_hms(tst_dt))) %&amp;gt;% # convert `test date` variable to a date 
  update_role(contains(&amp;quot;id&amp;quot;), ncessch, new_role = &amp;quot;id vars&amp;quot;) %&amp;gt;% # declare ID variables
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %&amp;gt;% # remove variables with zero variances
  step_novel(all_nominal()) %&amp;gt;% # prepares test data to handle previously unseen factor levels 
  step_unknown(all_nominal()) %&amp;gt;% # categorizes missing categorical data (NA&amp;#39;s) as `unknown`
  step_medianimpute(all_numeric(), -all_outcomes(), -has_role(&amp;quot;id vars&amp;quot;))  %&amp;gt;% # replaces missing numeric observations with the median
  step_dummy(all_nominal(), -has_role(&amp;quot;id vars&amp;quot;)) # dummy codes categorical variables&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;create-a-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Create a model&lt;/h1&gt;
&lt;p&gt;The last step before bringing in our data is to specify our model. This will call upon functions from the &lt;code&gt;{parsnip}&lt;/code&gt; package, which standardizes language for specifying a multitude of statistical models. There are a few core elements that you will need to specify for each model&lt;/p&gt;
&lt;div id=&#34;the-type-of-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The type of model&lt;/h2&gt;
&lt;p&gt;This indicates what type of model you choose to fit, each of which will be a different function. We’ll be focusing on decision tree methods using &lt;code&gt;bag_tree()&lt;/code&gt;, &lt;code&gt;random_forest()&lt;/code&gt;, and &lt;code&gt;boost_tree()&lt;/code&gt;. A full list of models can be found here &lt;a href=&#34;https://www.tidymodels.org/find/parsnip/&#34; class=&#34;uri&#34;&gt;https://www.tidymodels.org/find/parsnip/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-engine&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The engine&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;set_engine()&lt;/code&gt; calls the package to support the model you specified above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-mode&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The mode&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;set_mode()&lt;/code&gt; indicates the type of prediction you’d like to use in your model, you’ll choose between regression and classification. Since we are looking to predict student scores, which is a continuous predictor, we’ll be choosing regression.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-arguments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The arguments&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;set_args()&lt;/code&gt; allows you to set values for various parameters for your model, each model type will have a specific set of parameters that can be altered. For these parameters, you can either set a particular value or you can use the tune function to search for the optimal value of each parameter. Tuning requires a few extra steps, so we will leave the default arguments for clarity. For more information on tuning check out &lt;a href=&#34;https://tune.tidymodels.org/&#34; class=&#34;uri&#34;&gt;https://tune.tidymodels.org/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;create-a-workflow&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Create a workflow&lt;/h1&gt;
&lt;p&gt;Up to this point we’ve been setting up a lot of individual elements and now it is time to combine them to create a cohesive framework, called a &lt;em&gt;workflow&lt;/em&gt;, so we can run our desired models. First, we’ll use the &lt;code&gt;workflow()&lt;/code&gt; command and then we’ll pull in the recipe and model we already created. The next section shows three examples of specifying models and creating a workflow for different decision tree methods.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-examples&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model Examples&lt;/h1&gt;
&lt;div id=&#34;bagged-trees&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bagged trees&lt;/h2&gt;
&lt;p&gt;A bagged tree approach creates multiple subsets of data from the training set which are randomly chosen with replacement. Each subset of data is used to train a given decision tree. In the end, we have an ensemble of different models. The predictions from all the different trees are averaged together, giving us a stronger prediction than one tree could independently.&lt;/p&gt;
&lt;div id=&#34;specify-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Specify model&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(100)
mod_bag &amp;lt;- bag_tree() %&amp;gt;%
  set_mode(&amp;quot;regression&amp;quot;) %&amp;gt;%
  set_engine(&amp;quot;rpart&amp;quot;, times = 10) # 10 bootstrap resamples&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;create-workflow&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Create workflow&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wflow_bag &amp;lt;- workflow() %&amp;gt;% 
  add_recipe(rec) %&amp;gt;%
  add_model(mod_bag)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the model&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(100)
plan(multisession)

fit_bag &amp;lt;- fit_resamples(
  wflow_bag,
  cv,
  metrics = metric_set(rmse, rsq),
  control = control_resamples(verbose = TRUE,
                              save_pred = TRUE,
                              extract = function(x) extract_model(x)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualize&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualize&lt;/h3&gt;
&lt;p&gt;The plot below shows the root nodes from a bagged tree made of 100 trees (10 folds x 10 bootstrapped resamples). Root nodes are the 1st node in a decision tree, and they are determined by which variable best optimizes a loss function (e.g., minimizes mean square error [MSE] for continuous outcomes or Gini Index for categorical outcomes). Put roughly, the most common root nodes can be thought of as the most “important” predictors.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-06-22-tidymodels-decision-tree-learning-in-r/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random forest&lt;/h2&gt;
&lt;p&gt;Random forest is similar to bagged tree methodology but goes one step further. In addition to taking random subsets of data, the model also draws a random selection of features. Instead of utilizing all features, the random subset of features allows more predictors to be eligible root nodes. This is particularly useful for handling high dimensionality data (e.g., have more variables than participants/cases).&lt;/p&gt;
&lt;div id=&#34;specify-the-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Specify the model&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(100)
mod_rf &amp;lt;-rand_forest() %&amp;gt;%
  set_engine(&amp;quot;ranger&amp;quot;,
             num.threads = parallel::detectCores(), 
             importance = &amp;quot;permutation&amp;quot;, 
             verbose = TRUE) %&amp;gt;% 
  set_mode(&amp;quot;regression&amp;quot;) %&amp;gt;% 
  set_args(trees = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;create-workflow-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Create workflow&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wflow_rf &amp;lt;- workflow() %&amp;gt;% 
  add_model(mod_rf) %&amp;gt;% 
  add_recipe(rec)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the model&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(100)
plan(multisession)

fit_rf &amp;lt;- fit_resamples(
  wflow_rf,
  cv,
  metrics = metric_set(rmse, rsq),
  control = control_resamples(verbose = TRUE,
                              save_pred = TRUE,
                              extract = function(x) x)
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualize-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualize&lt;/h3&gt;
&lt;p&gt;The plot below shows the root nodes from a random forest with 1000 trees (specified using &lt;code&gt;set_args(trees = 1000)&lt;/code&gt; in the parsnip model object).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-06-22-tidymodels-decision-tree-learning-in-r/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;boosted-trees&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Boosted trees&lt;/h2&gt;
&lt;p&gt;Boosted trees, like bagged trees, are an ensemble model. Instead of applying successive models to resampled data and pooling estimates, boosted trees fit the next tree to the residuals (i.e., error term) of the prior tree. The goal is to minimize residual error through multiple trees, and is typically done with fairly “shallow” decision tree (i.e., 1-6 splits in each tree). Though each model is only slightly improving the error rate, the sequential use of many shallow trees makes computationally efficient (i.e. reduced run time) and highly accurate predictions.&lt;/p&gt;
&lt;div id=&#34;specify-the-model-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Specify the model&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod_boost &amp;lt;- boost_tree() %&amp;gt;% 
  set_engine(&amp;quot;xgboost&amp;quot;, nthreads = parallel::detectCores()) %&amp;gt;% 
  set_mode(&amp;quot;regression&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;create-workflow-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Create workflow&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wflow_boost &amp;lt;- workflow() %&amp;gt;% 
  add_recipe(rec) %&amp;gt;% 
  add_model(mod_boost)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the model&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(100)
plan(multisession)

fit_boost &amp;lt;- fit_resamples(
  wflow_boost, 
  cv,
  metrics = metric_set(rmse, rsq),
  control = control_resamples(verbose = TRUE,
                              save_pred = TRUE)
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualize-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualize&lt;/h3&gt;
&lt;p&gt;One of the few downfalls of &lt;code&gt;{tidymodels}&lt;/code&gt; is its (current) inability to plot these tree-based models. For the past two models, it was simpler to extract root nodes and plot them, but their interpretation (as we’re fitting to residuals instead of data sets) are not straightforward. For that reason, we don’t have any pretty plots here. Instead, we’ll skip to evaluating the metrics of all models.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluate-metrics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Evaluate metrics&lt;/h1&gt;
&lt;p&gt;After running these three models, it’s time to evaluate their performance. We can do this with &lt;code&gt;tune::collect_metrics()&lt;/code&gt;. The table below shows the estimate of the out-of-sample performance for each of our 3 models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;collect_metrics(fit_bag) %&amp;gt;% 
  bind_rows(collect_metrics(fit_rf)) %&amp;gt;%
  bind_rows(collect_metrics(fit_boost)) %&amp;gt;% 
  filter(.metric == &amp;quot;rmse&amp;quot;) %&amp;gt;% 
  mutate(model = c(&amp;quot;bag&amp;quot;, &amp;quot;rf&amp;quot;, &amp;quot;boost&amp;quot;)) %&amp;gt;% 
  select(model, everything()) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;model&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;.metric&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;.estimator&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std_err&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;bag&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;rmse&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;standard&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;98.71479&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.981677&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;rf&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;rmse&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;standard&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;95.43958&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.137891&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;boost&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;rmse&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;standard&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;96.42956&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.946813&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here, we are faced with a common problem in the machine learning world: choosing between models that perform similarly. Whether we would prefer random forests, bagged trees, or boosted trees may depend on computational efficiency (i.e., time) or other factors. In practice, tuning several hyperparameters may have made one model clearly preferable over the others, but in our case - relying on all defaults - we would probably have similar performance with both models on a new data set and would prefer random forest or boosted tree models for their efficiency.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;out-of-sample-performance&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Out-of-sample performance&lt;/h1&gt;
&lt;p&gt;The final step is to apply each trained model to our test data using &lt;code&gt;last_fit()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# bagged trees
final_fit_bag &amp;lt;- last_fit(
  wflow_bag,
  split = split
)

# random forest
final_fit_rf &amp;lt;- last_fit(
  wflow_rf,
  split = split
)

# boosted trees
final_fit_boost &amp;lt;- last_fit(
  wflow_boost,
  split = split
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The table below shows the actual out-of-sample performance for each of our 3 models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# show performance on test data
collect_metrics(final_fit_bag) %&amp;gt;% 
  bind_rows(collect_metrics(final_fit_rf)) %&amp;gt;%
  bind_rows(collect_metrics(final_fit_boost)) %&amp;gt;% 
  filter(.metric == &amp;quot;rmse&amp;quot;) %&amp;gt;% 
  mutate(model = c(&amp;quot;bag&amp;quot;, &amp;quot;rf&amp;quot;, &amp;quot;boost&amp;quot;)) %&amp;gt;% 
  select(model, everything()) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;model&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;.metric&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;.estimator&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;.estimate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;bag&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;rmse&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;standard&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;92.29708&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;rf&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;rmse&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;standard&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;89.63881&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;boost&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;rmse&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;standard&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;92.64633&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;After applying our 3 trained models to the unseen test data, it looks like random forest is the winner since it has the lowest RMSE. In this example, we only used 1% of the data to train these models, which could make it difficult to meaningfully compare their performance. However, the random forest model also results in the best out-of-sample prediction (RMSE = 84.08) when using all of the available data, which we did for the &lt;a href=&#34;https://www.kaggle.com/c/edld-654-spring-2020/leaderboard&#34;&gt;Kaggle competition&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
